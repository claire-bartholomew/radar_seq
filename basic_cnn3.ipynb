{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c pytorch pytorch-cpu torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up sequences of simple cell movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_cell_shift(row):\n",
    "    '''Create sequence of arrays depicting a cell moving at constant speed\n",
    "       across a domain, frame by frame'''\n",
    "    input_arrays = []\n",
    "    arr = np.zeros((100, 100))\n",
    "    for i, step in enumerate([10, 15, 20]):\n",
    "        arr2 = np.copy(arr)\n",
    "        arr2[row:row+10, step:step+10] = 1\n",
    "        #plt.pcolormesh(arr2)\n",
    "        #plt.show()\n",
    "        #plt.close()\n",
    "        input_arrays.append(arr2)\n",
    "        \n",
    "    input_arrays = np.array(input_arrays)\n",
    "\n",
    "    return input_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_cell_shift_v(column):\n",
    "    '''Create sequence of arrays depicting a cell moving at constant speed\n",
    "       across a domain, frame by frame'''\n",
    "    input_arrays = []\n",
    "    arr = np.zeros((100, 100))\n",
    "    for i, step in enumerate([10, 15, 20]):\n",
    "        arr2 = np.copy(arr)\n",
    "        arr2[step:step+10, column:column+10] = 1\n",
    "        #plt.pcolormesh(arr2)\n",
    "        #plt.show()\n",
    "        #plt.close()\n",
    "        input_arrays.append(arr2)\n",
    "        \n",
    "    input_arrays = np.array(input_arrays)\n",
    "\n",
    "    return input_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for row in range(0, 50, 5): #10):\n",
    "    dataset.append(create_basic_cell_shift(row))\n",
    "for column in range(0, 50, 5):\n",
    "    dataset.append(create_basic_cell_shift_v(column))\n",
    "dataset = np.array(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create torch dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "# Convert to torch tensors\n",
    "tensor = torch.stack([torch.Tensor(i) for i in dataset])\n",
    "#tensor = tensor.unsqueeze(0) # to add a fake batch dimension as torch.nn only supports inputs that are a mini-batch of samples, not a single sample\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = utils.DataLoader(tensor, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "dataset3 = []\n",
    "for column in range(75, 80):\n",
    "    dataset3.append(create_basic_cell_shift_v(column))\n",
    "for row in range(55, 60):\n",
    "    dataset3.append(create_basic_cell_shift(row))\n",
    "\n",
    "dataset3 = np.array(dataset3) \n",
    "tensor3 = torch.stack([torch.Tensor(i) for i in dataset3])\n",
    "print(tensor3.shape)\n",
    "\n",
    "val_loader = utils.DataLoader(tensor3, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.as_super = super(CNN, self)\n",
    "        self.as_super.__init__()\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv2d(2, 1, kernel_size=13, stride=1, padding=6)\n",
    "        self.pool_1 = torch.nn.MaxPool2d(kernel_size=3, stride=1, padding=1, return_indices=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        orig = x\n",
    "                \n",
    "        x = self.conv_1(x)\n",
    "        x, inds = self.pool_1(x)\n",
    "        \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def createLossAndOptimizer(net, learning_rate=0.01):\n",
    "    \n",
    "    #Loss function\n",
    "    loss = torch.nn.MSELoss()\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return(loss, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_net(net, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    #Print the hyperparameters of the training:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get training data\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Create the loss and optimizer functions\n",
    "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
    "    \n",
    "    #Time for printing at end how long training takes\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            #Get inputs\n",
    "            inputs, labels = data[:,:2], data[:,2]\n",
    "            \n",
    "            #Wrap them in a Variable object\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Set gradients to zero (as the backward function accumulates gradients so want a fresh one for each mini-batch)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs[0], labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            running_loss += loss_size.data.item() #[0]\n",
    "            total_train_loss += loss_size.data.item() #[0]\n",
    "            \n",
    "            #Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, \n",
    "                        time.time() - start_time))\n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for data in val_loader:\n",
    "            \n",
    "            #data = data.type('torch.FloatTensor')\n",
    "            inputs, labels = data[:,:2], data[:,2]\n",
    "            #Wrap tensors in Variables\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs[0], labels)\n",
    "            total_val_loss += val_loss_size.data.item() #[0]\n",
    "            \n",
    "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 2\n",
      "epochs= 10\n",
      "learning_rate= 0.001\n",
      "==============================\n",
      "Epoch 1, 15% \t train_loss: 0.02 took: 0.06s\n",
      "Epoch 1, 30% \t train_loss: 0.01 took: 0.10s\n",
      "Epoch 1, 45% \t train_loss: 0.01 took: 0.10s\n",
      "Epoch 1, 60% \t train_loss: 0.01 took: 0.19s\n",
      "Epoch 1, 75% \t train_loss: 0.01 took: 0.21s\n",
      "Epoch 1, 90% \t train_loss: 0.01 took: 0.10s\n",
      "Validation loss = 0.01\n",
      "Epoch 2, 15% \t train_loss: 0.01 took: 0.20s\n",
      "Epoch 2, 30% \t train_loss: 0.01 took: 0.11s\n",
      "Epoch 2, 45% \t train_loss: 0.01 took: 0.10s\n",
      "Epoch 2, 60% \t train_loss: 0.01 took: 0.09s\n",
      "Epoch 2, 75% \t train_loss: 0.00 took: 0.09s\n",
      "Epoch 2, 90% \t train_loss: 0.00 took: 0.10s\n",
      "Validation loss = 0.00\n",
      "Epoch 3, 15% \t train_loss: 0.01 took: 0.02s\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "train_net(cnn, batch_size=2, n_epochs=10, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_outputs(net, loader):\n",
    "    for i, data in enumerate(loader):\n",
    "        data = data.type('torch.FloatTensor')\n",
    "        inputs, labels = data[:,:2], data[:,2]\n",
    "        #Wrap tensors in Variables\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        #Forward pass\n",
    "        val_outputs = net(inputs)\n",
    "        \n",
    "        fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "        for i in range(2):\n",
    "            ax = fig.add_subplot(2, 3, i+1)\n",
    "            cf = plt.contourf(inputs[0,i], cmap=plt.cm.Greys)\n",
    "            ax.set_xticks(np.arange(0, 100, 10))\n",
    "            ax.set_yticks(np.arange(0, 100, 10))\n",
    "            plt.grid()\n",
    "            plt.setp(ax.xaxis.get_ticklabels(), visible=False)\n",
    "            plt.setp(ax.yaxis.get_ticklabels(), visible=False)\n",
    "            if i == 0:\n",
    "                plt.title('inputs')\n",
    "            \n",
    "        ax = fig.add_subplot(2, 3, 3)\n",
    "        cf = plt.contourf(labels[0], cmap=plt.cm.Greys)\n",
    "        ax.set_xticks(np.arange(0, 100, 10))\n",
    "        ax.set_yticks(np.arange(0, 100, 10))\n",
    "        plt.grid()\n",
    "        plt.setp(ax.xaxis.get_ticklabels(), visible=False)\n",
    "        plt.setp(ax.yaxis.get_ticklabels(), visible=False)\n",
    "        plt.title('truth')\n",
    "        ax = fig.add_subplot(2, 3, 6)\n",
    "        cf = plt.contourf(val_outputs[0, 0].detach().numpy(), cmap=plt.cm.Greys)\n",
    "        ax.set_xticks(np.arange(0, 100, 10))\n",
    "        ax.set_yticks(np.arange(0, 100, 10))\n",
    "        plt.grid()\n",
    "        plt.setp(ax.xaxis.get_ticklabels(), visible=False)\n",
    "        plt.setp(ax.yaxis.get_ticklabels(), visible=False)\n",
    "        plt.title('model')\n",
    "\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_outputs(cnn, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
